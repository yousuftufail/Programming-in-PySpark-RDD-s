{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2602f4",
   "metadata": {},
   "source": [
    "# Some info:\n",
    "I am going to run two examples for the construction of RDD and their outputs. These two examples are taken from DataCamp (Programming in PySpark RDD’s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6e540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all the required libraries\n",
    "\n",
    "#------------------------------------------------------------\n",
    "import findspark\n",
    "findspark.init(\"C:\\Program Files\\spark-3.1.2-bin-hadoop3.2\")\n",
    "\n",
    "\"\"\"RDD.collect command wont work without above two commands. It is my opinion to add above two commands (always)\n",
    "before executing anything in pyspark.\"\"\"\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef017d",
   "metadata": {},
   "source": [
    "# First RDD:\n",
    "In this excercise I am going to read the raw data using parralleize method. And then computing the cube of each data point using map function. At the end all the generated cubes are displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f8e2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n"
     ]
    }
   ],
   "source": [
    "# Construction of RDD:\n",
    "numbRDD=sc.parallelize(range(1, 10))\n",
    "\n",
    "\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "\tprint(numb)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80909097",
   "metadata": {},
   "source": [
    "# Counclusion: \n",
    "All cubes are upto the mark \n",
    "\n",
    "# Coution!\n",
    "\n",
    "collect() should only be used to retrieve results for small datasets. It shouldn’t be used on large datasets.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53fdcf",
   "metadata": {},
   "source": [
    "# Second RDD\n",
    "\n",
    "In this RDD I am going to read the textfile using .textfile method and trying to find a word \"spark\" in each line of the text\n",
    "file. These lines are filtered using filter function along with lambda function. A normal way of pyhton. I have also calculated that how many times a word \"spark\" is used. Finally, the first four lines having a word \"spark\" are displayed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d205c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 5\n",
      "Examples for Learning Spark\n",
      "Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file in the mini-complete-example directory.\n",
      "These examples have been updated to run against Spark 1.3 so they may be slightly different than the versions in your copy of \"Learning Spark\".\n",
      "Spark 1.3\n"
     ]
    }
   ],
   "source": [
    "file_path=\"readme.txt\"\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "  print(line) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591638e0",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "Note that the filter() operation does not mutate the existing fileRDD. Instead, it returns a pointer to an entirely new RDD. This can be verified by printing the types of fileRDD and fileRDD_filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b60c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c190416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
